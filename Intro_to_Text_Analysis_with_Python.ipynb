{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5k1gMHnj2Mr08kA+VPdAU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ar19z/TextAnalysis/blob/main/Intro_to_Text_Analysis_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U6pUQ_Plog8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Welcome to Intro to Text Analysis with Python!**\n",
        "Today, we're going to act as literary detectives. We have a collection of 26 classic British novels, and we'll use a Python \"recipe\" to uncover some interesting patterns within them."
      ],
      "metadata": {
        "id": "HZLHFFYEltuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Setup Our Kitchen\n",
        "First, we need to get our kitchen ready. This code block does three things:\n",
        "\n",
        "Imports the specialized tools (libraries) we need.\n",
        "\n",
        "Downloads the raw ingredients (our 26 novels) from the web.\n",
        "\n",
        "Unzips the file so we can access the novels.\n",
        "\n",
        "Click the \"play\" button on the left to run this cell."
      ],
      "metadata": {
        "id": "acDehUxul70w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import our \"kitchen tools\" (libraries)\n",
        "import requests      # For downloading files from the internet\n",
        "import zipfile       # For unzipping files\n",
        "import io            # For handling the downloaded data\n",
        "import os            # For navigating files and folders\n",
        "import shutil        # For copying, moving, deleting, and archiving files or folders\n",
        "import re            # For removing punctuation using regular expressions\n",
        "import nltk          # The main tool for natural language processing\n",
        "import matplotlib.pyplot as plt # For making charts and graphs\n",
        "\n",
        "# This command downloads the specific tools we need from NLTK\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print(\"Setup complete! All tools and ingredients are ready.\")\n",
        "\n",
        "USER   = \"ar19z\"\n",
        "REPO   = \"TextAnalysis\"\n",
        "BRANCH = \"b9fcd9ddb42004cca0804868ae2d4650cee3c4d4\"  # commit hash\n",
        "SUBDIR = \"A_Small_Collection_of_British_Fiction-master/corpus\"\n",
        "\n",
        "# Download the repo at that commit\n",
        "url = f\"https://github.com/{USER}/{REPO}/archive/{BRANCH}.zip\"\n",
        "r = requests.get(url)\n",
        "r.raise_for_status()\n",
        "\n",
        "# Clean old target\n",
        "target = \"/content/corpus\"\n",
        "if os.path.exists(target):\n",
        "    shutil.rmtree(target)\n",
        "os.makedirs(target, exist_ok=True)\n",
        "\n",
        "# Extract only SUBDIR\n",
        "with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
        "    prefix = f\"{REPO}-{BRANCH}/{SUBDIR}/\"\n",
        "    names = [n for n in z.namelist() if n.startswith(prefix)]\n",
        "    if not names:\n",
        "        raise SystemExit(f\"Folder '{SUBDIR}' not found. Check the path.\")\n",
        "\n",
        "    for n in names:\n",
        "        if n.endswith(\"/\"):\n",
        "            continue\n",
        "        rel = n[len(prefix):]             # strip the prefix\n",
        "        out_path = os.path.join(target, rel)\n",
        "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "        with z.open(n) as src, open(out_path, \"wb\") as dst:\n",
        "            dst.write(src.read())\n",
        "\n",
        "print(\"Extracted to:\", target)\n",
        "print(\"Count:\", len([f for f in os.listdir(target) if f.endswith('.txt')]))\n",
        "print(\"Sample:\", sorted(os.listdir(target))[:5])"
      ],
      "metadata": {
        "id": "Qvj1LdvHmHzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Point to Our Ingredients\n",
        "Now we need to tell our recipe where to find the ingredients. We've placed the novels in a folder called text_data/26_novels.\n",
        "\n",
        "If you wanted to use your *own* .txt files, you would upload them to Colab, put them in a folder, and just change the path in the line below!"
      ],
      "metadata": {
        "id": "DJ0YmvBoqM3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <<< STUDENTS: THIS IS THE LINE YOU WOULD CHANGE FOR YOUR OWN TEXTS >>>\n",
        "folder_path = \"/content/corpus\"\n",
        "\n",
        "# Let's list the files to make sure they're there\n",
        "file_list = os.listdir(folder_path)\n",
        "print(f\"Found {len(file_list)} files in the folder.\")\n",
        "print(\"Here are the first 5:\")\n",
        "print(file_list[:5])"
      ],
      "metadata": {
        "id": "yzIle2MQqLsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Prep the Ingredients (Preprocessing)\n",
        "Raw text is messy! It has capital letters, punctuation, and lots of common \"filler\" words (like 'the', 'a', 'is', 'of'). To get a good analysis, we need to clean it up. This is our \"text preprocessing\" step.\n",
        "\n",
        "This block of code will:\n",
        "\n",
        "\n",
        "\n",
        "1.   Read each text file.\n",
        "2.   Make all text lowercase.\n",
        "3.   Break the text into a list of individual words (tokenize).\n",
        "4.   Remove punctuation and common English stopwords.\n",
        "\n",
        "The result will be a clean list of \"significant\" words for each novel."
      ],
      "metadata": {
        "id": "iwNBF7_1qiHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Get the list of common English \"filler\" words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# We'll store all our processed words from all novels in this one giant list\n",
        "all_processed_words = []\n",
        "# We'll also store the original, raw text for later\n",
        "raw_text_corpus = \"\"\n",
        "\n",
        "\n",
        "print(\"Processing all novels... this might take a moment.\")\n",
        "\n",
        "# Loop through every file in our folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.txt'): # Make sure we're only reading text files\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            text = f.read()\n",
        "            raw_text_corpus += text # Add the raw text to our corpus for later\n",
        "\n",
        "            # 1. Make text lowercase\n",
        "            text = text.lower()\n",
        "\n",
        "            # 2. Remove punctuation (keeps only letters and spaces)\n",
        "            text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "            # 3. Tokenize (split text into a list of words)\n",
        "            words = word_tokenize(text)\n",
        "\n",
        "            # 4. Remove stopwords and short words\n",
        "            processed_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
        "\n",
        "            # Add the cleaned words from this book to our master list\n",
        "            all_processed_words.extend(processed_words)\n",
        "\n",
        "print(f\"\\n Processing complete!\")\n",
        "print(f\"We have a total of {len(all_processed_words)} significant words across all 26 novels.\")\n",
        "print(\"\\nHere's a sample of the first 20 processed words:\")\n",
        "print(all_processed_words[:20])"
      ],
      "metadata": {
        "id": "eCOOKh2-t-uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: The First Course - Word Frequency\n",
        "Now that our ingredients are prepped, let's do our first analysis! We'll count every single word to see which ones appear most often across all the novels.\n",
        "\n",
        "We'll use NLTK's FreqDist (Frequency Distribution) tool to do the counting and then plot the top 20 words on a bar chart."
      ],
      "metadata": {
        "id": "StlU2sO4Hs42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use NLTK's FreqDist to count the words\n",
        "freq_dist = nltk.FreqDist(all_processed_words)\n",
        "\n",
        "# Let's look at the 20 most common words\n",
        "most_common_words = freq_dist.most_common(20)\n",
        "\n",
        "print(\"Top 20 Most Common Words:\")\n",
        "for word, count in most_common_words:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# Now, let's visualize this!\n",
        "# We need to separate the words and their counts for the chart\n",
        "words_to_plot, counts_to_plot = zip(*most_common_words)\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(12, 6)) # Make the chart a bit bigger\n",
        "plt.bar(words_to_plot, counts_to_plot, color='skyblue')\n",
        "plt.title('Top 20 Most Frequent Words in 26 British Novels')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency (Count)')\n",
        "plt.xticks(rotation=45) # Rotate the x-axis labels so they don't overlap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T84jwZJGHwBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: The Main Course - Finding Context (Concordance)\n",
        "A word count is useful, but context is everything. How are these words actually being used? A concordance shows us every occurrence of a given word, along with the words that surround it.\n",
        "\n",
        "This allows us to see patterns in how language is used. We'll load our raw, unprocessed text into a special NLTK object to generate the concordance.\n",
        "\n",
        "Try changing the word in .concordance(\"money\") to other words like \"love\", \"man\", \"woman\", or \"house\" and re-running the cell!"
      ],
      "metadata": {
        "id": "R3zi3_cKH5yW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to create an NLTK Text object from our raw text to use the concordance tool\n",
        "# First, we tokenize the raw corpus\n",
        "raw_tokens = nltk.word_tokenize(raw_text_corpus)\n",
        "nltk_text = nltk.Text(raw_tokens)\n",
        "\n",
        "print(\"Displaying concordance for the word 'money':\\n\")\n",
        "# The concordance function will print its own output\n",
        "nltk_text.concordance(\"money\", width=80, lines=25)\n",
        "\n",
        "print(\"\\n\\nNow, let's try 'love':\\n\")\n",
        "nltk_text.concordance(\"love\", width=80, lines=25)"
      ],
      "metadata": {
        "id": "zvMHbFgTH-2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# Load each .txt file separately\n",
        "files = sorted(glob.glob(\"/content/corpus/*.txt\"))\n",
        "\n",
        "for file in files:\n",
        "    with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        raw_text = f.read()\n",
        "\n",
        "    tokens = nltk.word_tokenize(raw_text)\n",
        "    nltk_text = nltk.Text(tokens)\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Concordance for 'money' in {file}:\\n\")\n",
        "    nltk_text.concordance(\"money\", width=80, lines=5)  # adjust lines as needed\n",
        "\n",
        "    print(\"\\nConcordance for 'love':\\n\")\n",
        "    nltk_text.concordance(\"love\", width=80, lines=5)\n",
        "    print(\"=\"*80, \"\\n\")"
      ],
      "metadata": {
        "id": "cD9sFbNrKfTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, nltk, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure tokenizer is available\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "CORPUS_DIR = \"/content/corpus\"  # change if needed\n",
        "files = sorted(glob.glob(os.path.join(CORPUS_DIR, \"*.txt\")))\n",
        "assert files, f\"No .txt files found in {CORPUS_DIR}\"\n",
        "\n",
        "rows = []\n",
        "for fp in files:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        raw = f.read()\n",
        "    tokens = [t.lower() for t in nltk.word_tokenize(raw)]\n",
        "    total = len(tokens)\n",
        "\n",
        "    love_count  = sum(1 for t in tokens if t == \"love\")\n",
        "    money_count = sum(1 for t in tokens if t == \"money\")\n",
        "\n",
        "    # Metrics\n",
        "    diff  = love_count - money_count\n",
        "    ratio = love_count / (money_count if money_count > 0 else 1e-9)  # avoid div-by-zero\n",
        "    love_per_10k  = (love_count  / total) * 10000 if total else 0\n",
        "    money_per_10k = (money_count / total) * 10000 if total else 0\n",
        "\n",
        "    rows.append({\n",
        "        \"title\": Path(fp).stem,\n",
        "        \"path\": fp,\n",
        "        \"tokens\": total,\n",
        "        \"love\": love_count,\n",
        "        \"money\": money_count,\n",
        "        \"love_minus_money\": diff,\n",
        "        \"love_to_money_ratio\": ratio,\n",
        "        \"love_per_10k\": love_per_10k,\n",
        "        \"money_per_10k\": money_per_10k\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Winners\n",
        "by_diff  = df.sort_values(\"love_minus_money\", ascending=False).reset_index(drop=True)\n",
        "by_ratio = df[df[\"money\"] > 0].sort_values(\"love_to_money_ratio\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"=== Winner by difference (love - money) ===\")\n",
        "print(by_diff.loc[0, [\"title\", \"love\", \"money\", \"love_minus_money\"]], \"\\n\")\n",
        "\n",
        "if not by_ratio.empty:\n",
        "    print(\"=== Winner by ratio (love / money) ===\")\n",
        "    print(by_ratio.loc[0, [\"title\", \"love\", \"money\", \"love_to_money_ratio\"]], \"\\n\")\n",
        "else:\n",
        "    print(\"No occurrences of 'money' found; ratio ranking not applicable.\\n\")\n",
        "\n",
        "print(\"Top 10 by difference:\")\n",
        "display(by_diff[[\"title\",\"love\",\"money\",\"love_minus_money\",\"love_per_10k\",\"money_per_10k\"]].head(10))\n",
        "\n",
        "print(\"Top 10 by ratio (excluding zero-money):\")\n",
        "if not by_ratio.empty:\n",
        "    display(by_ratio[[\"title\",\"love\",\"money\",\"love_to_money_ratio\",\"love_per_10k\",\"money_per_10k\"]].head(10))\n",
        "else:\n",
        "    print(\"(none)\")"
      ],
      "metadata": {
        "id": "dG8Ud3lXLKRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, nltk, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Make sure NLTK tokenizer is ready\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# === Set your words here ===\n",
        "WORD1 = \"chicken\"\n",
        "WORD2 = \"alfredo\"\n",
        "\n",
        "CORPUS_DIR = \"/content/corpus\"  # change if needed\n",
        "files = sorted(glob.glob(os.path.join(CORPUS_DIR, \"*.txt\")))\n",
        "assert files, f\"No .txt files found in {CORPUS_DIR}\"\n",
        "\n",
        "rows = []\n",
        "for fp in files:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        raw = f.read()\n",
        "    tokens = [t.lower() for t in nltk.word_tokenize(raw)]\n",
        "    total = len(tokens)\n",
        "\n",
        "    w1_count = sum(1 for t in tokens if t == WORD1.lower())\n",
        "    w2_count = sum(1 for t in tokens if t == WORD2.lower())\n",
        "\n",
        "    diff  = w1_count - w2_count\n",
        "    ratio = w1_count / (w2_count if w2_count > 0 else 1e-9)  # avoid div-by-zero\n",
        "    w1_per_10k = (w1_count / total) * 10000 if total else 0\n",
        "    w2_per_10k = (w2_count / total) * 10000 if total else 0\n",
        "\n",
        "    rows.append({\n",
        "        \"title\": Path(fp).stem,\n",
        "        \"tokens\": total,\n",
        "        WORD1: w1_count,\n",
        "        WORD2: w2_count,\n",
        "        f\"{WORD1}_minus_{WORD2}\": diff,\n",
        "        f\"{WORD1}_to_{WORD2}_ratio\": ratio,\n",
        "        f\"{WORD1}_per_10k\": w1_per_10k,\n",
        "        f\"{WORD2}_per_10k\": w2_per_10k\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Ranking by difference and ratio\n",
        "by_diff  = df.sort_values(f\"{WORD1}_minus_{WORD2}\", ascending=False).reset_index(drop=True)\n",
        "by_ratio = df[df[WORD2] > 0].sort_values(f\"{WORD1}_to_{WORD2}_ratio\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(f\"=== Winner by difference ({WORD1} - {WORD2}) ===\")\n",
        "print(by_diff.loc[0, [\"title\", WORD1, WORD2, f\"{WORD1}_minus_{WORD2}\"]], \"\\n\")\n",
        "\n",
        "if not by_ratio.empty:\n",
        "    print(f\"=== Winner by ratio ({WORD1} / {WORD2}) ===\")\n",
        "    print(by_ratio.loc[0, [\"title\", WORD1, WORD2, f\"{WORD1}_to_{WORD2}_ratio\"]], \"\\n\")\n",
        "else:\n",
        "    print(f\"No occurrences of '{WORD2}' found; ratio ranking not applicable.\\n\")\n",
        "\n",
        "print(\"Top 10 by difference:\")\n",
        "display(by_diff.head(10))\n",
        "\n",
        "print(\"Top 10 by ratio (excluding zero-counts for second word):\")\n",
        "if not by_ratio.empty:\n",
        "    display(by_ratio.head(10))\n",
        "else:\n",
        "    print(\"(none)\")"
      ],
      "metadata": {
        "id": "KVdJzvOlMqbB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}